data=read.table(file="DataTP4.txt",header=TRUE)
names(data)
summary(data)
dim(data)data$color = as.factor(data$color)
data$color = as.factor(data$color)
summary(data)
lm.out1=lm(quality~alcohol,data=data)
summary(lm.out1)
x11()
points(data$quality,col="blue",pch="+")
abline(fitted(lm.out1),col="red")
x11()
plot(data$quality,type ="l",lwd=2,main="indice de qualitÃ©",xlab="Date",ylab="quality")
abline(lm.out1,col="red")
x11()
plot(data$alcohol,data$quality,xlab="alcohol",ylab="quality",pch='+')
abline(lm.out1,col="red")
x11()
plot(data$quality,type ="l",lwd=2,main="indice de quality",xlab="Date",ylab="quality")
points(data$alcohol,col="blue",pch="+")
points(fitted(lm.out1),col="red",pch="+")
legend(0,268,lty=1,col=c("black"),legend=c("quality"),bty="n")
legend(0,256,pch="+",col=c("blue","red"),legend=c("       alcohol","       ASsimple"),bty="n") 
x11()
par(mfrow=c(1,2))
hist(data$quality)
hist(data$vin)
x11()
par(mfrow=c(1,2))
hist(data$quality)
hist(data$color)
score(data$quality,data$color)
source("scores.R")
score(data$quality,data$color)
scores(data$quality,data$color)
var3.test(data$quality)
var.test(data$quality)
x11()
par(mfrow=c(1,2))
hist(data$quality)
hist(data$color)
cor
?cor
lm.out2=lm(quality~facid + vacid + cacid + sugar+  chlorides+ fsulf +tsulf+ density +pH +sulphates +alcohol +quality +color,data)
lm.out2=lm(quality~facid + vacid + cacid + sugar+  chlorides+ fsulf +tsulf+ density +pH +sulphates +alcohol +color,data)
summary(lm.out2)
legend(0,268,lty=1,col=c("black"),legend=c("quality"),bty="n")
legend(0,256,pch="+",col=c("blue","red"),legend=c("       alcohol","       ASsimple"),bty="n") 
legend(0,268,lty=1,col=c("black"),legend=c("quality"),bty="n")
legend(0,256,pch="+",col=c("blue","red"),legend=c("       alcohol","       ASsimple"),bty="n") 
library(MASS)
lm.outAIC=stepAIC(lm.out2)
summary(lm.outAIC)
formula(lm.outAIC)
lm.outint=lm(quality~.*.,data)
summary(lm.outint)
lm.outAICint=stepAIC(lm.outint)
summary(lm.outAICint)
formula(lm.outAICint)
lm.outBICint=stepAIC(lm.outint,k=log(nrow(data)))
summary(lm.outBICint)
formula(lm.outBICint)
lm.outAICint=stepAIC(lm.outint)
summary(lm.outAICint)
formula(lm.outAICint)
lm.outBICint=stepAIC(lm.outint,k=log(nrow(data)))
summary(lm.outBICint)
formula(lm.outBICint)
?stepAIC
lm.outAICint=stepAIC(lm.outint)
lm.outINT=stepAIC(lm.outint,k=10)
summary(lm.outINT)
formula(lm.outINT)
?stepAIC
dim(lm.outAICint)
dim(lm.outBICint)
lm.outINT=stepAIC(lm.outint,10)
plot(data$quality[1:200],type ="l",main="quality",xlab="Indice",ylab="indice de quality")
points(fitted(lm.outAICint),col="red",pch="+",cex=1.2)
x11()
plot(data$quality[1:200],type ="l",main="quality",xlab="Indice",ylab="indice de quality")
points(fitted(lm.outAICint),col="red",pch="+",cex=1.2)
score(data$quality,fitted(lm.outAIC))
# biais=11.82 RMSE=38.34
# -> la prevision brute est biaisee
score(data$quality,fitted(lm.outAICint))
# biais=0 RMSE=33.01
# -> la regression simple debiaise la prevision brute et reduit la variabilite de l'erreur
score(data$quality,fitted(lm.outBICint))
score=function(obs,prev) {
rmse=sqrt(mean((prev-obs)**2))
biais=mean(prev-obs)
print("Biais  RMSE") 
return(round(c(biais,rmse),3))
}
score(data$quality,fitted(lm.outAIC))
# biais=11.82 RMSE=38.34
# -> la prevision brute est biaisee
score(data$quality,fitted(lm.outAICint))
# biais=0 RMSE=33.01
# -> la regression simple debiaise la prevision brute et reduit la variabilite de l'erreur
score(data$quality,fitted(lm.outBICint))
nappr=ceiling(0.8*nrow(data))
ii=sample(1:nrow(data),nappr)
jj=setdiff(1:nrow(data),ii)
datatest=data[jj,]
datapp=data[ii,]
lm.outAIC=lm(formula(lm.outAIC),datapp)
lm.outAICint=lm(formula(lm.outAICint),datapp)
lm.outBICint=lm(formula(lm.outBICint),datapp)
score(datapp$quality,fitted(lm.outAIC)) 
score(datapp$quality,fitted(lm.outAIC)) 
score(datapp$quality,fitted(lm.outAICint))
score(datapp$quality,fitted(lm.outBICint))
score(datatest$quality,predict(lm.outAIC,datatest))
score(datatest$quality,predict(lm.outAICint,datatest))
score(datatest$quality,predict(lm.outBICint,datatest))
library(MASS)
data=read.table(file="DataTP4.txt",header=TRUE)
RMSE=function(obs,pr){
return(sqrt(mean((pr-obs)^2)))}
# Choix automatique de predicteurs a partir du modele complet sans puis avec interactions
lm.outBIC=stepAIC(lm(quality~.,data),k=log(nrow(data)),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),k=log(nrow(data)),trace=0)
k=100
tab=matrix(nrow=k,ncol=8)
for (i in 1:k) {
nappr=ceiling(0.8*nrow(data))
ii=sample(1:nrow(data),nappr)
jj=setdiff(1:nrow(data),ii)
datatest=data[jj,]
datapp=data[ii,]
# Estimation des modeles
regsimple=lm(quality~alcohol,datapp)
lm.outBIC=lm(formula(lm.outBIC),datapp)
lm.outBICint=lm(formula(lm.outBICint),datapp)
regsurap=lm(quality~.*.*.,datapp) # modele volontairement sur-apprenti (interactions d'ordre 3), sans selection automatique des predicteurs
# Scores sur apprentissage
tab[i,1]=RMSE(datapp$quality,predict(regsimple))
tab[i,2]=RMSE(datapp$quality,predict(lm.outBIC))
tab[i,3]=RMSE(datapp$quality,predict(lm.outBICint))
tab[i,4]=RMSE(datapp$quality,predict(regsurap))
# Scores sur test
tab[i,5]=RMSE(datatest$quality,predict(regsimple,datatest))
tab[i,6]=RMSE(datatest$quality,predict(lm.outBIC,datatest))
tab[i,7]=RMSE(datatest$quality,predict(lm.outBICint,datatest))
tab[i,8]=RMSE(datatest$quality,predict(regsurap,datatest))
}
x11()
boxplot(tab,col=c(rep("blue",4),rep("red",4)),xlab="bleu=apprentissage - rouge=test",
names=c("ASsimp","ASbic","ASbicint","AScomplexe","ASsimp","ASbic","ASbicint","AScomplexe"),main="Modele lineaire gaussien - Score RMSE")
library(MASS)
data=read.table(file="DataTP4.txt",header=TRUE)
RMSE=function(obs,pr){
return(sqrt(mean((pr-obs)^2)))}
# Choix automatique de predicteurs a partir du modele complet sans puis avec interactions
lm.outAIC=stepAIC(lm(quality~.,data),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),k=log(nrow(data)),trace=0)
k=100
tab=matrix(nrow=k,ncol=8)
for (i in 1:k) {
nappr=ceiling(0.8*nrow(data))
ii=sample(1:nrow(data),nappr)
jj=setdiff(1:nrow(data),ii)
datatest=data[jj,]
datapp=data[ii,]
# Estimation des modeles
lm.outAIC=lm(formula(lm.outAIC),datapp)
lm.outAICint=lm(formula(lm.outAICint),datapp)
lm.outBICint=lm(formula(lm.outBICint),datapp)
regsurap=lm(quality~.*.*.,datapp) # modele volontairement sur-apprenti (interactions d'ordre 3), sans selection automatique des predicteurs
# Scores sur apprentissage
tab[i,1]=RMSE(datapp$quality,predict(lm.outAIC))
tab[i,2]=RMSE(datapp$quality,predict(lm.outAICint))
tab[i,3]=RMSE(datapp$quality,predict(lm.outBICint))
tab[i,4]=RMSE(datapp$quality,predict(regsurap))
# Scores sur test
tab[i,5]=RMSE(datatest$quality,predict(lm.outAIC,datatest))
tab[i,6]=RMSE(datatest$quality,predict(lm.outAICint,datatest))
tab[i,7]=RMSE(datatest$quality,predict(lm.outBICint,datatest))
tab[i,8]=RMSE(datatest$quality,predict(regsurap,datatest))
}
x11()
boxplot(tab,col=c(rep("blue",4),rep("red",4)),xlab="bleu=apprentissage - rouge=test",
names=c("ASaic","ASaicint","ASbicint","AScomplexe","ASaic","ASaicint","ASbicint","AScomplexe"),main="Modele lineaire gaussien - Score RMSE")
library(MASS)
data=read.table(file="DataTP4.txt",header=TRUE)
RMSE=function(obs,pr){
return(sqrt(mean((pr-obs)^2)))}
# Choix automatique de predicteurs a partir du modele complet sans puis avec interactions
lm.outAIC=stepAIC(lm(quality~.,data),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),k=log(nrow(data)),trace=0)
k=100
tab=matrix(nrow=k,ncol=8)
for (i in 1:k) {
nappr=ceiling(0.8*nrow(data))
ii=sample(1:nrow(data),nappr)
jj=setdiff(1:nrow(data),ii)
datatest=data[jj,]
datapp=data[ii,]
# Estimation des modeles
lm.outAIC=lm(formula(lm.outAIC),datapp)
lm.outAICint=lm(formula(lm.outAICint),datapp)
lm.outBICint=lm(formula(lm.outBICint),datapp)
# Scores sur apprentissage
tab[i,1]=RMSE(datapp$quality,predict(lm.outAIC))
tab[i,2]=RMSE(datapp$quality,predict(lm.outAICint))
tab[i,3]=RMSE(datapp$quality,predict(lm.outBICint))
# Scores sur test
tab[i,5]=RMSE(datatest$quality,predict(lm.outAIC,datatest))
tab[i,6]=RMSE(datatest$quality,predict(lm.outAICint,datatest))
tab[i,7]=RMSE(datatest$quality,predict(lm.outBICint,datatest))
}
x11()
boxplot(tab,col=c(rep("blue",4),rep("red",4)),xlab="bleu=apprentissage - rouge=test",
names=c("ASaic","ASaicint","ASbicint","ASaic","ASaicint","ASbicint"),main="Modele lineaire gaussien - Score RMSE")
library(MASS)
data=read.table(file="DataTP4.txt",header=TRUE)
RMSE=function(obs,pr){
return(sqrt(mean((pr-obs)^2)))}
# Choix automatique de predicteurs a partir du modele complet sans puis avec interactions
lm.outAIC=stepAIC(lm(quality~.,data),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),k=log(nrow(data)),trace=0)
k=100
tab=matrix(nrow=k,ncol=8)
for (i in 1:k) {
nappr=ceiling(0.8*nrow(data))
ii=sample(1:nrow(data),nappr)
jj=setdiff(1:nrow(data),ii)
datatest=data[jj,]
datapp=data[ii,]
# Estimation des modeles
lm.outAIC=lm(formula(lm.outAIC),datapp)
lm.outAICint=lm(formula(lm.outAICint),datapp)
lm.outBICint=lm(formula(lm.outBICint),datapp)
# Scores sur apprentissage
tab[i,1]=RMSE(datapp$quality,predict(lm.outAIC))
tab[i,2]=RMSE(datapp$quality,predict(lm.outAICint))
tab[i,3]=RMSE(datapp$quality,predict(lm.outBICint))
# Scores sur test
tab[i,5]=RMSE(datatest$quality,predict(lm.outAIC,datatest))
tab[i,6]=RMSE(datatest$quality,predict(lm.outAICint,datatest))
tab[i,7]=RMSE(datatest$quality,predict(lm.outBICint,datatest))
}
x11()
boxplot(tab,col=c(rep("blue",3),rep("red",3)),xlab="bleu=apprentissage - rouge=test",
names=c("ASaic","ASaicint","ASbicint","ASaic","ASaicint","ASbicint"),main="Modele lineaire gaussien - Score RMSE")
library(MASS)
data=read.table(file="DataTP4.txt",header=TRUE)
RMSE=function(obs,pr){
return(sqrt(mean((pr-obs)^2)))}
# Choix automatique de predicteurs a partir du modele complet sans puis avec interactions
lm.outAIC=stepAIC(lm(quality~.,data),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),trace=0)
lm.outBICint=stepAIC(lm(quality~.*.,data),k=log(nrow(data)),trace=0)
k=100
tab=matrix(nrow=k,ncol=8)
for (i in 1:k) {
nappr=ceiling(0.8*nrow(data))
ii=sample(1:nrow(data),nappr)
jj=setdiff(1:nrow(data),ii)
datatest=data[jj,]
datapp=data[ii,]
# Estimation des modeles
lm.outAIC=lm(formula(lm.outAIC),datapp)
lm.outAICint=lm(formula(lm.outAICint),datapp)
lm.outBICint=lm(formula(lm.outBICint),datapp)
regsurap=lm(quality~.*.*.,datapp) # modele volontairement sur-apprenti (interactions d'ordre 3), sans selection automatique des predicteurs
# Scores sur apprentissage
tab[i,1]=RMSE(datapp$quality,predict(lm.outAIC))
tab[i,2]=RMSE(datapp$quality,predict(lm.outAICint))
tab[i,3]=RMSE(datapp$quality,predict(lm.outBICint))
tab[i,4]=RMSE(datapp$quality,predict(regsurap))
# Scores sur test
tab[i,5]=RMSE(datatest$quality,predict(lm.outAIC,datatest))
tab[i,6]=RMSE(datatest$quality,predict(lm.outAICint,datatest))
tab[i,7]=RMSE(datatest$quality,predict(lm.outBICint,datatest))
tab[i,8]=RMSE(datatest$quality,predict(regsurap,datatest))
}
x11()
boxplot(tab,col=c(rep("blue",4),rep("red",4)),xlab="bleu=apprentissage - rouge=test",
names=c("ASaic","ASaicint","ASbicint","AScomplexe","ASaic","ASaicint","ASbicint","AScomplexe"),main="Modele lineaire gaussien - Score RMSE")
q()
